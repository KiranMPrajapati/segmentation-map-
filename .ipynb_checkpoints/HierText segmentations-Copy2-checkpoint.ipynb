{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brilliant-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "velvet-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/mnt/researchteam/document_understanding/dataset/train'\n",
    "train_csv_file = '/mnt/researchteam/document_understanding/hiertext/gt/train.jsonl'\n",
    "val_data_dir = '/mnt/researchteam/document_understanding/dataset/validation'\n",
    "val_csv_file = '/mnt/researchteam/document_understanding/hiertext/gt/validation.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2deb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc3aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierText(Dataset):\n",
    "    def __init__(self, csv_file, data_dir, transform=None):\n",
    "        self.data = json.load(open(csv_file, 'r'))[\"annotations\"][:1000]\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def draw_mask(self, vertices, w, h):\n",
    "        mask = np.zeros((h, w, 3), dtype=np.float32)\n",
    "        mask = cv2.fillPoly(mask, [vertices], [1.] * 3)[:, :, 0]\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data_annotations = self.data[idx]\n",
    "        img_name = os.path.join(self.data_dir, data_annotations['image_id'])\n",
    "        image = cv2.imread(f\"{img_name}.jpg\")\n",
    "        w = data_annotations['image_width']\n",
    "        h = data_annotations['image_height']\n",
    "\n",
    "        gt_word_masks = []\n",
    "        gt_word_weights = []\n",
    "\n",
    "        for paragraph in data_annotations['paragraphs']:\n",
    "            for line in paragraph['lines']:\n",
    "                for word in line['words']:\n",
    "                    gt_word_weights.append(1.0 if word['legible'] else 0.0)\n",
    "                    vertices = np.array(word['vertices'])\n",
    "                    gt_word_mask = self.draw_mask(vertices, w, h)\n",
    "                    gt_word_masks.append(gt_word_mask)\n",
    "\n",
    "        n_mask = len(gt_word_masks)\n",
    "\n",
    "        gt_masks = (np.stack(gt_word_masks, -1) if n_mask else np.zeros(((h + 1) // 2, (w + 1) // 2, 0), np.float32))\n",
    "        gt_weights = (np.array(gt_word_weights) if n_mask else np.zeros((0,), np.float32))\n",
    "        \n",
    "        palette = [[1]]*n_mask\n",
    "        colored = np.reshape(np.matmul(np.reshape(gt_masks, (-1, n_mask)), palette), (h, w, 1))\n",
    "        dont_care_mask = (np.reshape(np.matmul(np.reshape(gt_masks, (-1, n_mask)), np.reshape(1.- gt_weights, (-1, 1))), (h, w, 1)) > 0).astype(np.float32)\n",
    "\n",
    "        binary_image = np.clip(dont_care_mask * 1. + (1. - dont_care_mask) * colored, 0., 1.)\n",
    "        \n",
    "        sample = {\"image\": image.astype(np.uint8), \"binary_image\": binary_image.astype(np.uint8)}\n",
    "\n",
    "        if self.transform:\n",
    "            sample[\"image\"] = self.transform(sample[\"image\"])\n",
    "            sample[\"binary_image\"] = self.transform(sample[\"binary_image\"])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc09434",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToPILImage(), transforms.Resize((400,400)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73bfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiertext_train_dataset = HierText(csv_file=train_csv_file, data_dir=train_data_dir, transform=transforms)\n",
    "hiertext_val_dataset = HierText(csv_file=val_csv_file, data_dir=val_data_dir, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05748346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "\n",
    "# for i in range(len(hiertext_train_dataset)):\n",
    "#     sample = hiertext_train_dataset[i]\n",
    "\n",
    "#     print(i, sample['image'].shape, sample['binary_image'].shape)\n",
    "    \n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#     ax1.imshow(sample['image'].permute(1,2,0))\n",
    "#     ax2.imshow(sample['binary_image'].permute(1,2,0))\n",
    "    \n",
    "#     if i ==3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c384a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "train_dataloader = DataLoader(hiertext_train_dataset, batch_size=bs, shuffle=False)\n",
    "val_dataloader = DataLoader(hiertext_val_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "other-brunswick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fda0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, data in enumerate(val_dataloader):\n",
    "#     print(batch_idx, data['image'].shape, data['binary_image'].shape)\n",
    "    \n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#     ax1.imshow(data['image'][0].permute(1,2,0))\n",
    "#     ax2.imshow(data['binary_image'][0].permute(1,2,0))\n",
    "#     if batch_idx == 3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240a6c4",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a458d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU()\n",
    "#             )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "#         self.layer7 = nn.Sequential(\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU())\n",
    "\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, ceil_mode=True)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=256*25*25, out_features=512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer6(out)\n",
    "#         out = self.layer7(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer8(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe12785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=512, out_features=256*25*25)\n",
    "        \n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "#         self.layer5 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU())\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = out.view(out.size(0), 256, 25, 25)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c429d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75bbee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a758002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Encoder().to(device)\n",
    "decoder_model = Decoder().to(device)\n",
    "ip = torch.rand(1, 3, 400, 400).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "671ac79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(encoder_model, decoder_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128337d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "minor-roberts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86b6f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeda38c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "corrected-tampa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"saved_models/model4.pth\", map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "altered-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch_idx: 0, num_data: 1000, Loss: 0.5748628377914429\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(epoch): \n",
    "    total_loss = 0 \n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        image, binary_image = data[\"image\"].to(device), data[\"binary_image\"].to(device)\n",
    "        pred_binary_image = model(image)\n",
    "        loss = loss_fn(binary_image, pred_binary_image)\n",
    "        total_loss += loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Epoch: {e}, batch_idx: {batch_idx}, num_data: {len(train_dataloader.dataset)}, Loss: {loss}\")\n",
    "    epoch_loss = (total_loss.item()*bs)/len(train_dataloader.dataset)\n",
    "    print(f\"Epoch: {e}, Epoch Loss: {epoch_loss}\")\n",
    "    writer.add_scalar('Loss/train', epoch_loss, e)\n",
    "    if e % 10 == 0:\n",
    "        if os.path.exists('/mnt/researchteam/.local/share/Trash/'):\n",
    "            shutil.rmtree('/mnt/researchteam/.local/share/Trash/')            \n",
    "        if os.path.exists(f\"saved_models/model{e-10}.pth\"):\n",
    "            os.remove(f\"saved_models/model{e-10}.pth\")\n",
    "        torch.save(model.state_dict(), f\"saved_models/model{e}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36342755",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc-und",
   "language": "python",
   "name": "doc-und"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
